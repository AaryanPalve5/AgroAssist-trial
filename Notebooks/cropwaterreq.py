# -*- coding: utf-8 -*-
"""CropWaterReq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QzUru_EMXYddSK0Qt4Dw_rTjwDT0ZV7E
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv("/content/DATASET - Sheet1.csv")

df.head()

df.info()

df.isnull()

df.isnull().sum()

df.describe()

import seaborn as sns

sns.histplot(df['WATER REQUIREMENT'],bins=30 ,kde=True)
plt.xlim(0, 50)
plt.title('Distribution of Water Requirement')
plt.xlabel('Water requirement')
plt.ylabel('Frequency')
plt.show()

sns.boxplot(x='WEATHER CONDITION' , y='WATER REQUIREMENT' ,data=df)
plt.ylim(0, 15)
plt.title('Water Requirement by Weather Condition')
plt.xlabel('Weather Condition')
plt.ylabel('Water Requirement')
plt.show()

sns.barplot(x='TEMPERATURE', y='WATER REQUIREMENT', data=df)
plt.title('Water Requirement by Temperature')
plt.xlabel('Temperature Range')
plt.ylabel('Water Requirement')
plt.show()

data_encoded = pd.get_dummies(df, columns=['CROP TYPE', 'SOIL TYPE', 'REGION' , 'TEMPERATURE' , 'WEATHER CONDITION'])

correlation_matrix = data_encoded.corr()

plt.figure(figsize=(12,10))

sns.heatmap(correlation_matrix , annot=False ,cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Boxplot for each categorical variable
categorical_features = ['CROP TYPE', 'SOIL TYPE', 'REGION', 'TEMPERATURE', 'WEATHER CONDITION']

for feature in categorical_features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=feature, y='WATER REQUIREMENT', data=df)
    plt.xticks(rotation=90)
    plt.ylim(0, 50)
    plt.title(f'Water Requirement by {feature}')
    plt.show()

# One-hot encode categorical variables
data_encoded = pd.get_dummies(df, columns=['CROP TYPE', 'SOIL TYPE', 'REGION', 'TEMPERATURE', 'WEATHER CONDITION'])
data_encoded.head()

from scipy.stats import skew

# Identify numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Calculate skewness
skewness = df[numerical_cols].skew()
print(skewness)

df_log = df.copy()
if df['WATER REQUIREMENT'].min() > 0:  # Log transformation requires positive values
    df_log['WATER REQUIREMENT'] = np.log(df['WATER REQUIREMENT'])

df_sqrt = df.copy()
df_sqrt['WATER REQUIREMENT'] = np.sqrt(df['WATER REQUIREMENT'])

from scipy.stats import boxcox

df_boxcox = df.copy()
if df['WATER REQUIREMENT'].min() > 0:
    df_boxcox['WATER REQUIREMENT'], _ = boxcox(df['WATER REQUIREMENT'])

print("Log Transformation Skewness:\n", df_log['WATER REQUIREMENT'].skew())
print("Square Root Transformation Skewness:\n", df_sqrt['WATER REQUIREMENT'].skew())
print("Box-Cox Transformation Skewness:\n", df_boxcox['WATER REQUIREMENT'].skew())

df['WATER REQUIREMENT'] = df_boxcox['WATER REQUIREMENT']

sns.histplot(df['WATER REQUIREMENT'],bins=30 ,kde=True)
plt.xlim(0, 10)
plt.title('Distribution of Water Requirement')
plt.xlabel('Water requirement')
plt.ylabel('Frequency')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import xgboost as xgb

# Assuming df is your DataFrame

# Step 1: Convert temperature ranges to numerical values
def convert_temperature_range(temp_range):
    if isinstance(temp_range, str):
        try:
            low, high = temp_range.split('-')
            return (float(low) + float(high)) / 2
        except ValueError:
            return float(temp_range)  # Handle case where temperature is a single value
    elif isinstance(temp_range, (int, float)):
        return float(temp_range)
    else:
        raise ValueError(f"Unexpected data type: {type(temp_range)}")

df['TEMPERATURE'] = df['TEMPERATURE'].apply(convert_temperature_range)

# Step 2: Preprocess the categorical features
label_encoders = {}
for column in ['CROP TYPE', 'SOIL TYPE', 'REGION', 'WEATHER CONDITION']:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Step 3: Normalize numerical features
scaler = StandardScaler()
df['TEMPERATURE'] = scaler.fit_transform(df[['TEMPERATURE']])

# Step 4: Split the data
X = df.drop('WATER REQUIREMENT', axis=1)
y = df['WATER REQUIREMENT']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Build and evaluate models
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    print(f'{name} Mean Squared Error: {mse}')

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 1.0]
}

# Initialize the model
xgb_model = xgb.XGBRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Best parameters
print(f"Best parameters found: {grid_search.best_params_}")

# Predict using the best estimator
best_xgb_model = grid_search.best_estimator_
predictions = best_xgb_model.predict(X_test)

# Calculate MSE
mse = mean_squared_error(y_test, predictions)
print(f'XGBoost with Hyperparameter Tuning Mean Squared Error: {mse}')

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import xgboost as xgb



# Step 1: Convert temperature ranges to numerical values
def convert_temperature_range(temp_range):
    if isinstance(temp_range, str):
        try:
            low, high = temp_range.split('-')
            return (float(low) + float(high)) / 2
        except ValueError:
            return float(temp_range)  # Handle case where temperature is a single value
    elif isinstance(temp_range, (int, float)):
        return float(temp_range)
    else:
        raise ValueError(f"Unexpected data type: {type(temp_range)}")

df['TEMPERATURE'] = df['TEMPERATURE'].apply(convert_temperature_range)

# Step 2: Preprocess the categorical features
label_encoders = {}
for column in ['CROP TYPE', 'SOIL TYPE', 'REGION', 'WEATHER CONDITION']:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Step 3: Normalize numerical features
scaler = StandardScaler()
df['TEMPERATURE'] = scaler.fit_transform(df[['TEMPERATURE']])

# Step 4: Split the data
X = df.drop('WATER REQUIREMENT', axis=1)
y = df['WATER REQUIREMENT']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Build and evaluate models
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    print(f'{name} Mean Squared Error: {mse}')

# Hyperparameter tuning for XGBoost
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 1.0]
}

xgb_model = xgb.XGBRegressor(random_state=42)
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_xgb_model = grid_search.best_estimator_
predictions = best_xgb_model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
print(f'XGBoost with Hyperparameter Tuning Mean Squared Error: {mse}')

import pickle

target_variable = 'WATER REQUIREMENT'

X = df.drop(target_variable, axis=1)
y = df[target_variable]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Train the model
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_model.predict(X_test)

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Save the model using Pickle
model_filename = 'xgb_model.pbz'
with open(model_filename, 'wb') as f:
    pickle.dump(xgb_model, f)

print(f'Model saved as {model_filename}')



